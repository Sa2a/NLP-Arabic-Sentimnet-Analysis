{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"araBERT.ipynb","provenance":[],"mount_file_id":"1I_8J7mxEScw1TPrkO3YGOAqD3cpCmQ9m","authorship_tag":"ABX9TyMaB08LvTQkr/sEe4Woq7ww"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"587970842d64495990be3fe835f6fdd6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2214f31700144f3d91a5fcffa531c36a","IPY_MODEL_47b357b1e7234bb6bab5106568032ef1","IPY_MODEL_72715fb6b8a145cab8683fbb37396853"],"layout":"IPY_MODEL_65adc799c7c74a47a550bfb72d7fec54"}},"2214f31700144f3d91a5fcffa531c36a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_265569ce78f2455e8917522201c17c14","placeholder":"​","style":"IPY_MODEL_49344f01040d4dc594f28c06e3eeb9ea","value":"Downloading: 100%"}},"47b357b1e7234bb6bab5106568032ef1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_73b62d6d48e74ee79600a0575e3df339","max":611,"min":0,"orientation":"horizontal","style":"IPY_MODEL_699c7c18bd7e48c8a8ddd78a03a7c825","value":611}},"72715fb6b8a145cab8683fbb37396853":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40461dcae2ed4eb8b0620d3c740581ec","placeholder":"​","style":"IPY_MODEL_05b24fa7ce0c40159c284a93bb2de3f7","value":" 611/611 [00:00&lt;00:00, 18.0kB/s]"}},"65adc799c7c74a47a550bfb72d7fec54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"265569ce78f2455e8917522201c17c14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49344f01040d4dc594f28c06e3eeb9ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73b62d6d48e74ee79600a0575e3df339":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"699c7c18bd7e48c8a8ddd78a03a7c825":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40461dcae2ed4eb8b0620d3c740581ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05b24fa7ce0c40159c284a93bb2de3f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2f2499c48ee45c88e350397c28ae7e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_79507aad00044855bc62127120ef0e84","IPY_MODEL_67996ac3192a427d9df0f8b6ab11706f","IPY_MODEL_9e8cdc193c2d4f2d95b998976285ca6a"],"layout":"IPY_MODEL_e1921b20b3354fb7a9641a79bd8d5782"}},"79507aad00044855bc62127120ef0e84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5821d7facf14ab7a200f1d7abe7401b","placeholder":"​","style":"IPY_MODEL_394f240161ec40118e3e6c70b1331fba","value":"Downloading: 100%"}},"67996ac3192a427d9df0f8b6ab11706f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e08f3962c5ef4baabf1db5ba5dde36b3","max":384,"min":0,"orientation":"horizontal","style":"IPY_MODEL_57b6887593f04c55abf6229cdea453af","value":384}},"9e8cdc193c2d4f2d95b998976285ca6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecb28f225ef54dae8bb8270ac032e1fa","placeholder":"​","style":"IPY_MODEL_2a0a0a77f8d543ef807243a33b8f9788","value":" 384/384 [00:00&lt;00:00, 11.2kB/s]"}},"e1921b20b3354fb7a9641a79bd8d5782":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5821d7facf14ab7a200f1d7abe7401b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"394f240161ec40118e3e6c70b1331fba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e08f3962c5ef4baabf1db5ba5dde36b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57b6887593f04c55abf6229cdea453af":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ecb28f225ef54dae8bb8270ac032e1fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a0a0a77f8d543ef807243a33b8f9788":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74a453cf22f44aadbbd1f0c41c54f018":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_198b17ef8066470185393fcb7139c0db","IPY_MODEL_95d986f944a74d96b55dfab944ae7b18","IPY_MODEL_2700cc9b4e584d6bb7ac419b3bcc8718"],"layout":"IPY_MODEL_ab01567334c84e8cad7090b5c94c47b6"}},"198b17ef8066470185393fcb7139c0db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb4f8b39b8984f4d989cb9d0665130e5","placeholder":"​","style":"IPY_MODEL_3af91eab880a47398d85ad33c0c3bc9b","value":"Downloading: 100%"}},"95d986f944a74d96b55dfab944ae7b18":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a494b3385a0f4ff3991b74908555a009","max":719993,"min":0,"orientation":"horizontal","style":"IPY_MODEL_37fa126ad1844be3a8d03fb14ca11645","value":719993}},"2700cc9b4e584d6bb7ac419b3bcc8718":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f47a53297dad4032ba4ffd93f6ace570","placeholder":"​","style":"IPY_MODEL_7df926429d94493698f8cd340e84261f","value":" 703k/703k [00:01&lt;00:00, 561kB/s]"}},"ab01567334c84e8cad7090b5c94c47b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb4f8b39b8984f4d989cb9d0665130e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3af91eab880a47398d85ad33c0c3bc9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a494b3385a0f4ff3991b74908555a009":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37fa126ad1844be3a8d03fb14ca11645":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f47a53297dad4032ba4ffd93f6ace570":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7df926429d94493698f8cd340e84261f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe4fa8720fb04fb2bc02530b8b7c090e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e5c869e91f2c4609881fdfdcfc36b2ec","IPY_MODEL_ee1a9e6b4fc749d1a102e25a3682185c","IPY_MODEL_6b3cea6c4444461bb0a6d5f49b3ed505"],"layout":"IPY_MODEL_f840621893e24348abae70fc1ac3ef1d"}},"e5c869e91f2c4609881fdfdcfc36b2ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a92ba68926a49b09b1dee40074b4960","placeholder":"​","style":"IPY_MODEL_258e48397fec46ad8bb778315c0394ac","value":"Downloading: 100%"}},"ee1a9e6b4fc749d1a102e25a3682185c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e9aa669595e46dd891e3a1f49da55c3","max":2306039,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81200cc9e0d44d10bc91457af483d59e","value":2306039}},"6b3cea6c4444461bb0a6d5f49b3ed505":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d5f99a216744622a8f8f623c1e07f4d","placeholder":"​","style":"IPY_MODEL_5d6076dc0f044fbaaff85d502fa24145","value":" 2.20M/2.20M [00:01&lt;00:00, 1.92MB/s]"}},"f840621893e24348abae70fc1ac3ef1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a92ba68926a49b09b1dee40074b4960":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"258e48397fec46ad8bb778315c0394ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e9aa669595e46dd891e3a1f49da55c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81200cc9e0d44d10bc91457af483d59e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d5f99a216744622a8f8f623c1e07f4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d6076dc0f044fbaaff85d502fa24145":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c54585e5b42648dba307d643677d1165":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d378a46efd6f48c490bd6cd84653272e","IPY_MODEL_7c1f86c3a73240e299a0c7ef09e8cd47","IPY_MODEL_376f4f96e275432bbc573fd6d93a8b75"],"layout":"IPY_MODEL_457dc9ee248b43328500210f6b5805f4"}},"d378a46efd6f48c490bd6cd84653272e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54db849bd2e945a9b06dc7a48759022e","placeholder":"​","style":"IPY_MODEL_eb1e2b12acf9442caa0f1528e9f6c7a6","value":"Downloading: 100%"}},"7c1f86c3a73240e299a0c7ef09e8cd47":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c31a974a44d44b2aa48f700e72bcc98","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7730c737592a49498c0f0d9eccc56407","value":112}},"376f4f96e275432bbc573fd6d93a8b75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_97071c43ff064405b475a7cd19a46aa3","placeholder":"​","style":"IPY_MODEL_299383b7255740b4836695b6782a1422","value":" 112/112 [00:00&lt;00:00, 1.15kB/s]"}},"457dc9ee248b43328500210f6b5805f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54db849bd2e945a9b06dc7a48759022e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb1e2b12acf9442caa0f1528e9f6c7a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c31a974a44d44b2aa48f700e72bcc98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7730c737592a49498c0f0d9eccc56407":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"97071c43ff064405b475a7cd19a46aa3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"299383b7255740b4836695b6782a1422":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"243a36e8653a479caf280ac7d3e9ff25":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_99b1de05daa64f45b575478b36f35b80","IPY_MODEL_77b0571595d34e998f48dcebe096f478","IPY_MODEL_eadba4ab66414324ab3fd88873e940c6"],"layout":"IPY_MODEL_4506cee77dd54268aebf42c19db88031"}},"99b1de05daa64f45b575478b36f35b80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_418ad42b6c6742b29ff4cb846da2bc65","placeholder":"​","style":"IPY_MODEL_b29d52c9b41e477bbddbc8db66f60840","value":"Downloading: 100%"}},"77b0571595d34e998f48dcebe096f478":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b7c7361609643818c4d1e21f26f1c3c","max":543490667,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39333d7b21324bbe83f6f82256a7b0e3","value":543490667}},"eadba4ab66414324ab3fd88873e940c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98ad8470bbe94f648e50b487eb30a6ae","placeholder":"​","style":"IPY_MODEL_da6f62cfd1304395ae1ec366a6b13f6f","value":" 518M/518M [00:32&lt;00:00, 18.0MB/s]"}},"4506cee77dd54268aebf42c19db88031":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"418ad42b6c6742b29ff4cb846da2bc65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b29d52c9b41e477bbddbc8db66f60840":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b7c7361609643818c4d1e21f26f1c3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39333d7b21324bbe83f6f82256a7b0e3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98ad8470bbe94f648e50b487eb30a6ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da6f62cfd1304395ae1ec366a6b13f6f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["\n","%pip install transformers"],"metadata":{"id":"dUDrVwkSA3tp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657470827373,"user_tz":-120,"elapsed":11937,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}},"outputId":"387115d1-8ba7-4a69-d0e8-e6252c637116"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 36.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 57.3 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 10.1 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 54.4 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"]}]},{"cell_type":"code","source":["%store transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GblapEXF9SPH","executionInfo":{"status":"error","timestamp":1657319801141,"user_tz":-120,"elapsed":343,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}},"outputId":"1a71dc79-dc38-46c8-cc97-30a34fe66a9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["UsageError: Unknown variable 'transformers'\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n","\n","#============= Initialize Arabic Bert =============#\n","tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"aubmindlab/bert-base-arabertv2\", num_labels=3)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":320,"referenced_widgets":["587970842d64495990be3fe835f6fdd6","2214f31700144f3d91a5fcffa531c36a","47b357b1e7234bb6bab5106568032ef1","72715fb6b8a145cab8683fbb37396853","65adc799c7c74a47a550bfb72d7fec54","265569ce78f2455e8917522201c17c14","49344f01040d4dc594f28c06e3eeb9ea","73b62d6d48e74ee79600a0575e3df339","699c7c18bd7e48c8a8ddd78a03a7c825","40461dcae2ed4eb8b0620d3c740581ec","05b24fa7ce0c40159c284a93bb2de3f7","e2f2499c48ee45c88e350397c28ae7e2","79507aad00044855bc62127120ef0e84","67996ac3192a427d9df0f8b6ab11706f","9e8cdc193c2d4f2d95b998976285ca6a","e1921b20b3354fb7a9641a79bd8d5782","a5821d7facf14ab7a200f1d7abe7401b","394f240161ec40118e3e6c70b1331fba","e08f3962c5ef4baabf1db5ba5dde36b3","57b6887593f04c55abf6229cdea453af","ecb28f225ef54dae8bb8270ac032e1fa","2a0a0a77f8d543ef807243a33b8f9788","74a453cf22f44aadbbd1f0c41c54f018","198b17ef8066470185393fcb7139c0db","95d986f944a74d96b55dfab944ae7b18","2700cc9b4e584d6bb7ac419b3bcc8718","ab01567334c84e8cad7090b5c94c47b6","fb4f8b39b8984f4d989cb9d0665130e5","3af91eab880a47398d85ad33c0c3bc9b","a494b3385a0f4ff3991b74908555a009","37fa126ad1844be3a8d03fb14ca11645","f47a53297dad4032ba4ffd93f6ace570","7df926429d94493698f8cd340e84261f","fe4fa8720fb04fb2bc02530b8b7c090e","e5c869e91f2c4609881fdfdcfc36b2ec","ee1a9e6b4fc749d1a102e25a3682185c","6b3cea6c4444461bb0a6d5f49b3ed505","f840621893e24348abae70fc1ac3ef1d","5a92ba68926a49b09b1dee40074b4960","258e48397fec46ad8bb778315c0394ac","4e9aa669595e46dd891e3a1f49da55c3","81200cc9e0d44d10bc91457af483d59e","4d5f99a216744622a8f8f623c1e07f4d","5d6076dc0f044fbaaff85d502fa24145","c54585e5b42648dba307d643677d1165","d378a46efd6f48c490bd6cd84653272e","7c1f86c3a73240e299a0c7ef09e8cd47","376f4f96e275432bbc573fd6d93a8b75","457dc9ee248b43328500210f6b5805f4","54db849bd2e945a9b06dc7a48759022e","eb1e2b12acf9442caa0f1528e9f6c7a6","3c31a974a44d44b2aa48f700e72bcc98","7730c737592a49498c0f0d9eccc56407","97071c43ff064405b475a7cd19a46aa3","299383b7255740b4836695b6782a1422","243a36e8653a479caf280ac7d3e9ff25","99b1de05daa64f45b575478b36f35b80","77b0571595d34e998f48dcebe096f478","eadba4ab66414324ab3fd88873e940c6","4506cee77dd54268aebf42c19db88031","418ad42b6c6742b29ff4cb846da2bc65","b29d52c9b41e477bbddbc8db66f60840","9b7c7361609643818c4d1e21f26f1c3c","39333d7b21324bbe83f6f82256a7b0e3","98ad8470bbe94f648e50b487eb30a6ae","da6f62cfd1304395ae1ec366a6b13f6f"]},"id":"nXimcMkHALg6","executionInfo":{"status":"ok","timestamp":1657470885589,"user_tz":-120,"elapsed":58223,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}},"outputId":"fc8111d6-ce9e-4777-c01f-c2c238b31ccc"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/611 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"587970842d64495990be3fe835f6fdd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/384 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2f2499c48ee45c88e350397c28ae7e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/703k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74a453cf22f44aadbbd1f0c41c54f018"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/2.20M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe4fa8720fb04fb2bc02530b8b7c090e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c54585e5b42648dba307d643677d1165"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/518M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"243a36e8653a479caf280ac7d3e9ff25"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53846,"status":"ok","timestamp":1657470939428,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"},"user_tz":-120},"id":"A1t3hUZCRdVY","outputId":"5341b3c5-cbad-427e-f458-1ff9970b04e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["directory = \"/content/drive/MyDrive/DEBI/uottawa/Ds applications/kaggle competition/\"\n","\n","import numpy as np\n","import pandas as pd\n","import re\n","\n","# from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n","# directory = \"D:/DEBI/Uottawa/Data Science Applications/kaggle/\"\n","# tokenizer = AutoTokenizer.from_pretrained(directory+\"araBERT/araBERT_tokenizer\")\n","# model = AutoModelForSequenceClassification.from_pretrained(directory+\"araBERT/araBERT_Model\")\n","\n","data_frame = pd.read_csv(directory+\"araBERT/almost_cleaned_data.csv\")\n","\n","print(data_frame[\"class\"].value_counts())\n"],"metadata":{"id":"zEhVxHKW1t9e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657471019379,"user_tz":-120,"elapsed":3552,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}},"outputId":"e2845e65-92f0-4d0a-d527-cc331d5b1f10"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["neg    1389\n","pos    1389\n","neu    1389\n","Name: class, dtype: int64\n"]}]},{"cell_type":"code","source":["data_frame\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"PDYuVSMR_TaL","executionInfo":{"status":"ok","timestamp":1657471321165,"user_tz":-120,"elapsed":501,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}},"outputId":"1544f7aa-fd28-4a8e-97e2-baf75dd5168f"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                  tweet class\n","0     ما دام ورد ذكر حرف الدال ببدل بدي فانو الجوع ف...   neg\n","1     حمدا لله على سلامة أبنائنا المجندين المحررين و...   pos\n","2                                                 إحباط   neg\n","3     بالفيديو البابا تواضروس عن ماسبيرو حادث مثل أي...   neu\n","4     في ناس واقفة بشوم عند الشوارع المؤدية للتحرير ...   neg\n","...                                                 ...   ...\n","4162  ولا تكونوا كالذين نسوا الله ف أنساهم أنفسهم اس...   neu\n","4163  انشروا التذكير بالتكبير وأحيوا سنة هذه العشر ا...   neg\n","4164  شوقي ولد الهدى فالكائنات ضياء وفم الزمان تبسم ...   neu\n","4165  كلما رأيت أطفالا مشردين جائعين محاصرين لاجئين ...   neg\n","4166  أطلقوا اسم رابعة العدوية على مدرسة بالمغرب وحد...   neg\n","\n","[4167 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-e5918749-8f90-466c-85bd-51beefe8d799\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ما دام ورد ذكر حرف الدال ببدل بدي فانو الجوع ف...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>حمدا لله على سلامة أبنائنا المجندين المحررين و...</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>إحباط</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>بالفيديو البابا تواضروس عن ماسبيرو حادث مثل أي...</td>\n","      <td>neu</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>في ناس واقفة بشوم عند الشوارع المؤدية للتحرير ...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4162</th>\n","      <td>ولا تكونوا كالذين نسوا الله ف أنساهم أنفسهم اس...</td>\n","      <td>neu</td>\n","    </tr>\n","    <tr>\n","      <th>4163</th>\n","      <td>انشروا التذكير بالتكبير وأحيوا سنة هذه العشر ا...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>4164</th>\n","      <td>شوقي ولد الهدى فالكائنات ضياء وفم الزمان تبسم ...</td>\n","      <td>neu</td>\n","    </tr>\n","    <tr>\n","      <th>4165</th>\n","      <td>كلما رأيت أطفالا مشردين جائعين محاصرين لاجئين ...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>4166</th>\n","      <td>أطلقوا اسم رابعة العدوية على مدرسة بالمغرب وحد...</td>\n","      <td>neg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4167 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e5918749-8f90-466c-85bd-51beefe8d799')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e5918749-8f90-466c-85bd-51beefe8d799 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e5918749-8f90-466c-85bd-51beefe8d799');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["\n","from arabert.preprocess import ArabertPreprocessor\n","# from farasa.segmenter import FarasaSegmenter \n","# segmenter = FarasaSegmenter(interactive=True)\n","\n","model_name=\"bert-large-arabertv2\"\n","arabert_prep = ArabertPreprocessor(model_name=model_name)\n","text = \"ولن_نبالغ إذا قلنا إن هاتف أو كمبيوتر المكتب في زمننا هذا ضروري''\"\"\"\n","\n","print(arabert_prep.preprocess(text.strip(\"\\'\")))\n","# segmenter.segment(text)\n"],"metadata":{"executionInfo":{"status":"ok","timestamp":1657471306388,"user_tz":-120,"elapsed":27536,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}},"outputId":"470d0d02-f4d0-49e3-efe5-7fd6f33a19ff","colab":{"base_uri":"https://localhost:8080/"},"id":"Z37BWw1COGOp"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n","  InsecureRequestWarning)\n"]},{"output_type":"stream","name":"stdout","text":["100%|██████████| 241M/241M [00:17<00:00, 13.9MiB/s]\n"]},{"output_type":"stream","name":"stderr","text":["[2022-07-10 16:41:39,253 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"]},{"output_type":"stream","name":"stdout","text":["و+ لن _ نبالغ إذا قل +نا إن هاتف أو كمبيوتر ال+ مكتب في زمن +نا هذا ضروري\n"]}]},{"cell_type":"code","source":["# general_filter = lambda x: re.sub(r'([@A-Za-z0-9ـــــــــــــ]+)|[^\\w\\s]|#|http\\S+|[\\s]{2,}', '', x)\n","# _filter = lambda x: re.sub(r'_', ' ', x)\n","# rem_repeated_letters = lambda x: x[0:2] + ''.join([x[i] for i in range(2, len(x)) if x[i]!=x[i-1] or x[i]!=x[i-2]])\n","# heart_emotion_translate = lambda x: re.sub(r'[♥|❤️|❤|♡]+', 'قلب', x)\n","# happy_emotion_translate = lambda x: re.sub(r'(\\^[._]?\\^)+', 'سعيد', x)\n","# sad_emotion_translate = lambda x: re.sub(r'(-[._]?-)+', 'حزين', x)\n","\n","def pre_process(data_frame):\n","  data_frame_x = data_frame.copy()\n","  preProcessed = []   \n","  for x in data_frame_x.tweet:\n","      # x = x.strip(\"\\'\")\n","      # x = heart_emotion_translate(x)\n","      # x = happy_emotion_translate(x)\n","      # x = sad_emotion_translate(x)\n","      # x = general_filter(x)\n","      # x = _filter(x)\n","      # x = rem_repeated_letters(x)\n","      x = arabert_prep.preprocess(x)\n","      x = tokenizer(x).tokens()\n","      preProcessed.append(x)\n","  data_frame_x[\"araBERT\"] = preProcessed\n","  '''     \n","  # data_frame_x[\"araBERT_prep\"] = data_frame_x.tweet.apply(lambda x: x[2:-2])\n","  data_frame_x[\"araBERT_prep\"] = data_frame_x.tweet.apply(heart_emotion_translate)\n","  data_frame_x[\"araBERT_prep\"] = data_frame_x[\"araBERT_prep\"].apply(happy_emotion_translate)\n","  data_frame_x[\"araBERT_prep\"] = data_frame_x[\"araBERT_prep\"].apply(sad_emotion_translate)\n","  data_frame_x[\"araBERT_prep\"] = data_frame_x[\"araBERT_prep\"].apply(general_filter)\n","  data_frame_x[\"araBERT_prep\"] = data_frame_x[\"araBERT_prep\"].apply(_filter)\n","  data_frame_x[\"araBERT_prep\"] = data_frame_x[\"araBERT_prep\"].apply(rem_repeated_letters) \n","  data_frame_x[\"araBERT_prep\"] = data_frame_x[\"araBERT_prep\"].apply(lambda x: arabert_prep.preprocess(x))\n","  data_frame_x[\"araBERT_prep\"] = data_frame_x[\"araBERT_prep\"].apply(lambda x: tokenizer(x).tokens())\n","  '''\n","  return data_frame_x\n","\n","df = pre_process(data_frame)\n","\n","df\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"saIJaZ7OOyS3","executionInfo":{"status":"ok","timestamp":1657471342889,"user_tz":-120,"elapsed":8454,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}},"outputId":"27a6feff-70d8-4079-b303-7a12ba035fa8"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                  tweet class  \\\n","0     ما دام ورد ذكر حرف الدال ببدل بدي فانو الجوع ف...   neg   \n","1     حمدا لله على سلامة أبنائنا المجندين المحررين و...   pos   \n","2                                                 إحباط   neg   \n","3     بالفيديو البابا تواضروس عن ماسبيرو حادث مثل أي...   neu   \n","4     في ناس واقفة بشوم عند الشوارع المؤدية للتحرير ...   neg   \n","...                                                 ...   ...   \n","4162  ولا تكونوا كالذين نسوا الله ف أنساهم أنفسهم اس...   neu   \n","4163  انشروا التذكير بالتكبير وأحيوا سنة هذه العشر ا...   neg   \n","4164  شوقي ولد الهدى فالكائنات ضياء وفم الزمان تبسم ...   neu   \n","4165  كلما رأيت أطفالا مشردين جائعين محاصرين لاجئين ...   neg   \n","4166  أطلقوا اسم رابعة العدوية على مدرسة بالمغرب وحد...   neg   \n","\n","                                                araBERT  \n","0     [[CLS], ما, دام, ورد, ذكر, حرف, ال+, دال, ب+, ...  \n","1     [[CLS], حمد, +ا, ل+, الله, على, سلام, +ة, أبنا...  \n","2                                 [[CLS], إحباط, [SEP]]  \n","3     [[CLS], ب+, ال+, فيديو, ال+, بابا, تواضروس, عن...  \n","4     [[CLS], في, ناس, واقف, +ة, ب+, شوم, عند, ال+, ...  \n","...                                                 ...  \n","4162  [[CLS], و+, لا, تكون, +وا, ك+, الذين, نس, +وا,...  \n","4163  [[CLS], انشر, +وا, ال+, تذكير, ب+, ال+, تكبير,...  \n","4164  [[CLS], شوقي, ولد, ال+, هدى, ف+, ال+, كائن, +ا...  \n","4165  [[CLS], كلما, رأي, +ت, أطفال, +ا, مشرد, +ين, ج...  \n","4166  [[CLS], أطلق, +وا, اسم, رابع, +ة, ال+, عدوي, +...  \n","\n","[4167 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-73ca755d-edbf-41c0-8e4b-5c3c50cb6a44\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet</th>\n","      <th>class</th>\n","      <th>araBERT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ما دام ورد ذكر حرف الدال ببدل بدي فانو الجوع ف...</td>\n","      <td>neg</td>\n","      <td>[[CLS], ما, دام, ورد, ذكر, حرف, ال+, دال, ب+, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>حمدا لله على سلامة أبنائنا المجندين المحررين و...</td>\n","      <td>pos</td>\n","      <td>[[CLS], حمد, +ا, ل+, الله, على, سلام, +ة, أبنا...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>إحباط</td>\n","      <td>neg</td>\n","      <td>[[CLS], إحباط, [SEP]]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>بالفيديو البابا تواضروس عن ماسبيرو حادث مثل أي...</td>\n","      <td>neu</td>\n","      <td>[[CLS], ب+, ال+, فيديو, ال+, بابا, تواضروس, عن...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>في ناس واقفة بشوم عند الشوارع المؤدية للتحرير ...</td>\n","      <td>neg</td>\n","      <td>[[CLS], في, ناس, واقف, +ة, ب+, شوم, عند, ال+, ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4162</th>\n","      <td>ولا تكونوا كالذين نسوا الله ف أنساهم أنفسهم اس...</td>\n","      <td>neu</td>\n","      <td>[[CLS], و+, لا, تكون, +وا, ك+, الذين, نس, +وا,...</td>\n","    </tr>\n","    <tr>\n","      <th>4163</th>\n","      <td>انشروا التذكير بالتكبير وأحيوا سنة هذه العشر ا...</td>\n","      <td>neg</td>\n","      <td>[[CLS], انشر, +وا, ال+, تذكير, ب+, ال+, تكبير,...</td>\n","    </tr>\n","    <tr>\n","      <th>4164</th>\n","      <td>شوقي ولد الهدى فالكائنات ضياء وفم الزمان تبسم ...</td>\n","      <td>neu</td>\n","      <td>[[CLS], شوقي, ولد, ال+, هدى, ف+, ال+, كائن, +ا...</td>\n","    </tr>\n","    <tr>\n","      <th>4165</th>\n","      <td>كلما رأيت أطفالا مشردين جائعين محاصرين لاجئين ...</td>\n","      <td>neg</td>\n","      <td>[[CLS], كلما, رأي, +ت, أطفال, +ا, مشرد, +ين, ج...</td>\n","    </tr>\n","    <tr>\n","      <th>4166</th>\n","      <td>أطلقوا اسم رابعة العدوية على مدرسة بالمغرب وحد...</td>\n","      <td>neg</td>\n","      <td>[[CLS], أطلق, +وا, اسم, رابع, +ة, ال+, عدوي, +...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4167 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73ca755d-edbf-41c0-8e4b-5c3c50cb6a44')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-73ca755d-edbf-41c0-8e4b-5c3c50cb6a44 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-73ca755d-edbf-41c0-8e4b-5c3c50cb6a44');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["\n","from sklearn import preprocessing\n","\n","araBERT_prep = df[\"araBERT\"]\n","labels = df[\"class\"]\n","\n","# Apply label encoding over the labels\n","le = preprocessing.LabelEncoder()\n","Encodedlabels =le.fit_transform(labels)\n","\n","Encodedlabels\n","\n","\"\"\"# Padding and attention mask\"\"\"\n","\n","from keras.preprocessing.sequence import pad_sequences\n","\n","# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n","# In the original paper, the authors used a length of 512.\n","MAX_LEN = 128\n","# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in araBERT_prep]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask)\n","\n","print(input_ids[0])\n","print(attention_masks[0])\n","\n","\"\"\"# To tensors\"\"\"\n","\n","import tensorflow as tf\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","import numpy as np \n","# Use train_test_split to split our data into train and validation sets for training\n","percentage = 0.15\n","validationLen = round(percentage *len(input_ids))\n","\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, Encodedlabels,\n","                                        random_state=2018, test_size=0.1 ,stratify=Encodedlabels)\n","#input_ids[:-validationLen],input_ids[-validationLen:],Encodedlabels[:-validationLen],Encodedlabels[-validationLen:]\n","\n","train_masks, validation_masks, _,_ = train_test_split(attention_masks, input_ids,\n","                                        random_state=2018, test_size=0.1,stratify=Encodedlabels)\n","#attention_masks[:-validationLen],attention_masks[-validationLen:]\n","#\n","# Convert all of our data into torch tensors, the required datatype for our model\n","unique, counts = np.unique(train_masks, return_counts=True)\n","train_inputs = torch.tensor(train_inputs)\n","\n","validation_inputs = torch.tensor(validation_inputs)\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)\n","# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n","batch_size = 16\n","\n","# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n","# with an iterator the entire dataset does not need to be loaded into memory\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_dataloader = DataLoader(validation_data, batch_size=batch_size)\n","\n","for batch in train_dataloader:\n","  print(batch)\n","  break\n","\n","\"\"\"# Set optimizer parameters\"\"\"\n","\n","import torch.optim as optim\n","\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'gamma', 'beta']\n","optimizer_grouped_parameters=[{'params':[p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay_rate': 0.01},\n","                          {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay_rate': 0.0}]\n","# This variable contains all of the hyperparemeter information our training loop needs\n","#optimizer = BertAdam(optimizer_grouped_parameters,lr=2e-5,warmup=.1)\n","optimizer = optim.AdamW(optimizer_grouped_parameters,lr=2e-5)\n","\n","# loss = model(train_data[:1][0], attention_mask=train_data[:1][1], labels=train_data[:1][2])\n","# loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","# loss\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S7U2YwYeO3RP","executionInfo":{"status":"ok","timestamp":1657471373369,"user_tz":-120,"elapsed":3103,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}},"outputId":"03826eb1-cb22-4000-a3b5-a936b0fd18c4"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[  33  331 4838 2855  935 5372   20 7558    2 2098 8997 2410  218   20\n"," 7227  289 3326 2975   25   34    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0]\n","[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","[tensor([[   33,   985,   369,  ...,     0,     0,     0],\n","        [   33,  1195,   848,  ...,     0,     0,     0],\n","        [   33,    29,   913,  ...,     0,     0,     0],\n","        ...,\n","        [   33,   964,   539,  ...,     0,     0,     0],\n","        [   33,  9041, 26881,  ...,     0,     0,     0],\n","        [   33,   387,   539,  ...,     0,     0,     0]]), tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n","        [1., 1., 1.,  ..., 0., 0., 0.],\n","        [1., 1., 1.,  ..., 0., 0., 0.],\n","        ...,\n","        [1., 1., 1.,  ..., 0., 0., 0.],\n","        [1., 1., 1.,  ..., 0., 0., 0.],\n","        [1., 1., 1.,  ..., 0., 0., 0.]]), tensor([1, 0, 2, 1, 2, 2, 0, 1, 2, 1, 0, 0, 2, 1, 1, 2])]\n"]}]},{"cell_type":"code","source":["\n","\"\"\"# Training\"\"\"\n","\n","from tqdm import tqdm, trange\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","t = []\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# device = torch.device(\"cpu\")\n","# Store our loss and accuracy for plotting\n","train_loss_set = []\n","\n","# Number of training epochs \n","epochs = 5\n","\n","# Transfer the model to GPU\n","model.to(device)\n","\n","# trange is a tqdm wrapper around the normal python range\n","for _ in trange(epochs, desc=\"Epoch\"):\n","  \n","  \n","  # Training\n","  \n","  # Set our model to training mode (as opposed to evaluation mode)\n","  model.train()\n","  \n","  # Tracking variables\n","  tr_loss = 0\n","  nb_tr_examples, nb_tr_steps = 0, 0\n","  \n","  # Train the data for one epoch\n","  for step, batch in enumerate(train_dataloader):\n","    # Add batch to GPU\n","    b_input_ids, b_input_mask, b_labels = batch\n","\n","    # Clear out the gradients (by default they accumulate)\n","    optimizer.zero_grad()\n","\n","    # Forward pass\n","    loss = model(b_input_ids.to(device), token_type_ids=None, attention_mask=b_input_mask.to(device), labels=b_labels.to(device))[\"loss\"]\n","    train_loss_set.append(loss.item())\n","\n","    # Backward pass\n","    loss.backward()\n","    \n","    # Update parameters and take a step using the computed gradient\n","    optimizer.step()\n","    \n","    \n","    # Update tracking variables\n","    tr_loss += loss.item()\n","    nb_tr_examples += b_input_ids.size(0)\n","    nb_tr_steps += 1\n","\n","  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","    \n","  # Validation\n","\n","  # Put model in evaluation mode to evaluate loss on the validation set\n","  model.eval()\n","\n","  # Tracking variables \n","  eval_loss, eval_accuracy = 0, 0\n","  nb_eval_steps, nb_eval_examples = 0, 0\n","\n","  # Evaluate data for one epoch\n","  for batch in validation_dataloader:\n","    # Add batch to GPU\n","    # batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","    with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      logits = model(b_input_ids.to(device), token_type_ids=None, attention_mask=b_input_mask.to(device))\n","    \n","    # Move logits and labels to CPU\n","    logits = logits[\"logits\"].detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    \n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","\n","  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z3yxdE79PKjP","outputId":"529d7686-7d7d-4146-991b-53cb34a1e8fa","executionInfo":{"status":"ok","timestamp":1657473441551,"user_tz":-120,"elapsed":1846,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}}},"execution_count":34,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\rEpoch:   0%|          | 0/5 [00:00<?, ?it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train loss: 0.052120918039470276\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [01:26<05:46, 86.63s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.6805555555555556\n","Train loss: 0.04940952444845374\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:52<04:19, 86.35s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.6782407407407407\n","Train loss: 0.03937320415465597\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:18<02:52, 86.24s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.7013888888888888\n","Train loss: 0.038803109215532844\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:44<01:26, 86.12s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.6875\n","Train loss: 0.023236633477078633\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|██████████| 5/5 [07:10<00:00, 86.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.6851851851851852\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["df_submit = pd.read_csv(\"/content/drive/MyDrive/DEBI/uottawa/Ds applications/kaggle competition/araBERT/almost_cleaned_test.csv\")\n","\n","df_submit = pre_process(df_submit)\n","df_submit"],"metadata":{"id":"hYNUH0JaPhsr","colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"status":"ok","timestamp":1657474247810,"user_tz":-120,"elapsed":1315,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}},"outputId":"28b7ddd1-9bf4-4207-e956-010ad9bb9d60"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Id                                              tweet  \\\n","0      1                             فينو الأهبل ابن الأهبل   \n","1      2                   على المصريين وجمالهم ربنا يحميهم   \n","2      3  دول كثير جدا ودمهم خفيف العمارة التي أنا فيها ...   \n","3      4  أنا بعد هكذا خلي الذي يعنى بحاجة همضى على وصل ...   \n","4      5                                          أنا انتحر   \n","..   ...                                                ...   \n","682  683                        كاستيلو المنتزه بقا خره نيك   \n","683  684                                     أهم شيء الطاعة   \n","684  685             أيوه بقى يا مينو وحشتنا مينا فخر العرب   \n","685  686  العشري جون الزمالك الأول جول وأحمد الله لأنها ...   \n","686  687                                          كيف أسماء   \n","\n","                                               araBERT  \n","0      [[CLS], فينو, ال+, أهبل, ابن, ال+, أهبل, [SEP]]  \n","1    [[CLS], على, ال+, مصري, +ين, و+, جمال, +هم, رب...  \n","2    [[CLS], دول, كثير, جد, +ا, و+, دم, +هم, خفيف, ...  \n","3    [[CLS], أنا, بعد, هكذا, خلي, الذي, يعنى, ب+, ح...  \n","4                           [[CLS], أنا, انتحر, [SEP]]  \n","..                                                 ...  \n","682  [[CLS], كاستيلو, ال+, منتزه, بق, +ا, خر, +ه, ن...  \n","683             [[CLS], أهم, شيء, ال+, طاع, +ة, [SEP]]  \n","684  [[CLS], أيوه, بقى, يا, مينو, و+, حش, +ت, +نا, ...  \n","685  [[CLS], ال+, عشري, جون, ال+, زمالك, ال+, أول, ...  \n","686                         [[CLS], كيف, أسماء, [SEP]]  \n","\n","[687 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-2b6da7e9-3cad-416c-8e43-db1697d40a93\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>tweet</th>\n","      <th>araBERT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>فينو الأهبل ابن الأهبل</td>\n","      <td>[[CLS], فينو, ال+, أهبل, ابن, ال+, أهبل, [SEP]]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>على المصريين وجمالهم ربنا يحميهم</td>\n","      <td>[[CLS], على, ال+, مصري, +ين, و+, جمال, +هم, رب...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>دول كثير جدا ودمهم خفيف العمارة التي أنا فيها ...</td>\n","      <td>[[CLS], دول, كثير, جد, +ا, و+, دم, +هم, خفيف, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>أنا بعد هكذا خلي الذي يعنى بحاجة همضى على وصل ...</td>\n","      <td>[[CLS], أنا, بعد, هكذا, خلي, الذي, يعنى, ب+, ح...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>أنا انتحر</td>\n","      <td>[[CLS], أنا, انتحر, [SEP]]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>682</th>\n","      <td>683</td>\n","      <td>كاستيلو المنتزه بقا خره نيك</td>\n","      <td>[[CLS], كاستيلو, ال+, منتزه, بق, +ا, خر, +ه, ن...</td>\n","    </tr>\n","    <tr>\n","      <th>683</th>\n","      <td>684</td>\n","      <td>أهم شيء الطاعة</td>\n","      <td>[[CLS], أهم, شيء, ال+, طاع, +ة, [SEP]]</td>\n","    </tr>\n","    <tr>\n","      <th>684</th>\n","      <td>685</td>\n","      <td>أيوه بقى يا مينو وحشتنا مينا فخر العرب</td>\n","      <td>[[CLS], أيوه, بقى, يا, مينو, و+, حش, +ت, +نا, ...</td>\n","    </tr>\n","    <tr>\n","      <th>685</th>\n","      <td>686</td>\n","      <td>العشري جون الزمالك الأول جول وأحمد الله لأنها ...</td>\n","      <td>[[CLS], ال+, عشري, جون, ال+, زمالك, ال+, أول, ...</td>\n","    </tr>\n","    <tr>\n","      <th>686</th>\n","      <td>687</td>\n","      <td>كيف أسماء</td>\n","      <td>[[CLS], كيف, أسماء, [SEP]]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>687 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b6da7e9-3cad-416c-8e43-db1697d40a93')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2b6da7e9-3cad-416c-8e43-db1697d40a93 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2b6da7e9-3cad-416c-8e43-db1697d40a93');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["# df_submit[\"araBERT_prep\"] = pre_process(df_submit)\n","araBERT_prep_submit = df_submit[\"araBERT\"]"],"metadata":{"id":"YR_bmmcDtFFy","executionInfo":{"status":"ok","timestamp":1657474248406,"user_tz":-120,"elapsed":3,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n","# In the original paper, the authors used a length of 512.\n","MAX_LEN = 128\n","# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n","input_ids_submit = [tokenizer.convert_tokens_to_ids(x) for x in araBERT_prep_submit]\n","# Pad our input tokens\n","input_ids_submit = pad_sequences(input_ids_submit, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks_submit = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids_submit:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks_submit.append(seq_mask)"],"metadata":{"id":"p-3FbO25tFGl","executionInfo":{"status":"ok","timestamp":1657474249624,"user_tz":-120,"elapsed":3,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# Convert all of our data into torch tensors, the required datatype for our model\n","inputs_submit = torch.tensor(input_ids_submit)\n","masks_submit = torch.tensor(attention_masks_submit)\n","\n","# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n","# with an iterator the entire dataset does not need to be loaded into memory\n","batch_size = 16\n","submit_data = TensorDataset(inputs_submit, masks_submit)\n","\n","# do not use shuffle, we need the preds to be in same order\n","submit_dataloader = DataLoader(submit_data, batch_size=batch_size)#, shuffle=True)"],"metadata":{"id":"tFjj-Xs2tFGo","executionInfo":{"status":"ok","timestamp":1657474249625,"user_tz":-120,"elapsed":4,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# input, masks = next(iter(submit_dataloader))\n","# out = model(input, attention_mask=masks)[\"logits\"]\n","\n","# pred_flat = np.argmax(out.detach().numpy(), axis=1).flatten()\n","# le.inverse_transform(pred_flat)"],"metadata":{"id":"L0e9mjnMtFGq","executionInfo":{"status":"ok","timestamp":1657474249625,"user_tz":-120,"elapsed":3,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# Put the model in an evaluation state\n","model.eval()\n","\n","# Transfer model to GPU\n","model.to(device)\n","\n","outputs = []\n","for input, masks in submit_dataloader:\n","  torch.cuda.empty_cache() # empty the gpu memory\n","\n","  # Transfer the batch to gpu\n","  input = input.to(device)\n","  masks = masks.to(device)\n","\n","  # Run inference on the batch\n","  output = model(input, attention_mask=masks)[\"logits\"]\n","\n","  # Transfer the output to CPU again and convert to numpy\n","  output = output.cpu().detach().numpy()\n","\n","  # Store the output in a list\n","  outputs.append(output)\n","\n","# Concatenate all the lists within the list into one list\n","outputs = [x for y in outputs for x in y]\n","\n","# Inverse transform the label encoding\n","pred_flat = np.argmax(outputs, axis=1).flatten()\n","output_labels = le.inverse_transform(pred_flat)"],"metadata":{"id":"pqM28DMftFGs","executionInfo":{"status":"ok","timestamp":1657474256463,"user_tz":-120,"elapsed":6367,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["submission = pd.DataFrame({\"Id\":np.arange(1, len(output_labels)+1), \"class\":output_labels})\n","# save (submission)\n","submission.to_csv(\"/content/drive/MyDrive/DEBI/uottawa/Ds applications/kaggle competition/araBERT/submission_araBERT_2.csv\", index=False)"],"metadata":{"id":"7EFw5HmOtFGv","executionInfo":{"status":"ok","timestamp":1657474256464,"user_tz":-120,"elapsed":12,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["# Load data"],"metadata":{"id":"b1Zvu-ZH4hQa"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import re"],"metadata":{"id":"06JKQczq40pA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n","\n","directory = \"/content/drive/MyDrive/DEBI/uottawa/Ds applications/kaggle competition/\"\n","tokenizer = AutoTokenizer.from_pretrained(directory+\"araBERT/araBERT_tokenizer\")\n","model = AutoModelForSequenceClassification.from_pretrained(directory+\"araBERT/araBERT_Model\", num_labels=3)\n"],"metadata":{"id":"uBkAYtXE6PLD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_frame = pd.read_csv(directory+\"train.csv\")\n","data_frame.head(10)"],"metadata":{"id":"f1LsF5774dO0","executionInfo":{"status":"ok","timestamp":1657322123233,"user_tz":-120,"elapsed":8,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}},"outputId":"80ddef9e-0ffe-44d6-b973-06610f3c00fc","colab":{"base_uri":"https://localhost:8080/","height":363}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               tweet class\n","0  ' #علمتني_الحياه أن الذين يعيشون على الأرض ليس...   pos\n","1  ' #ميري_كرسمس كل سنة وانتم طيبين http://t.co/n...   pos\n","2                          ' و انتهى مشوار الخواجة '   neg\n","3                   ' مش عارف ابتدى مذاكره منين :/ '   neg\n","4  ' @mskhafagi  إختصروا الطريق بدلا من إختيار ال...   neg\n","5  ' إذ خانت المرأة فهي تبحث عن الإهتمام .. و إذ ...   neg\n","6    ' يا حبيبتي البعد ناار قلبي داب من الانتظار ♡ '   pos\n","7   ' \"\"\"يا سامحك الله، من علّمك أن تكون خيبة؟\"\"\"\" '   neg\n","8             ' خى مالها قفلت من كل حتة كده لييه ! '   neg\n","9  ' الناس اللى حضرت حفلات البرلمان القديم ياريت ...   neg"],"text/html":["\n","  <div id=\"df-c470b637-09e4-46d8-aadc-ddd4244d3625\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>' #علمتني_الحياه أن الذين يعيشون على الأرض ليس...</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>' #ميري_كرسمس كل سنة وانتم طيبين http://t.co/n...</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>' و انتهى مشوار الخواجة '</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>' مش عارف ابتدى مذاكره منين :/ '</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>' @mskhafagi  إختصروا الطريق بدلا من إختيار ال...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>' إذ خانت المرأة فهي تبحث عن الإهتمام .. و إذ ...</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>' يا حبيبتي البعد ناار قلبي داب من الانتظار ♡ '</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>' \"\"\"يا سامحك الله، من علّمك أن تكون خيبة؟\"\"\"\" '</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>' خى مالها قفلت من كل حتة كده لييه ! '</td>\n","      <td>neg</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>' الناس اللى حضرت حفلات البرلمان القديم ياريت ...</td>\n","      <td>neg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c470b637-09e4-46d8-aadc-ddd4244d3625')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c470b637-09e4-46d8-aadc-ddd4244d3625 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c470b637-09e4-46d8-aadc-ddd4244d3625');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["%pip install farasapy"],"metadata":{"id":"FIfXVWh75N6g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657471259901,"user_tz":-120,"elapsed":4525,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}},"outputId":"587c57d6-5837-410d-a63c-0777726dda8a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting farasapy\n","  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farasapy) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farasapy) (4.64.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy) (1.24.3)\n","Installing collected packages: farasapy\n","Successfully installed farasapy-0.0.14\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/aub-mind/arabert.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kFY9yMwX996k","executionInfo":{"status":"ok","timestamp":1657471246900,"user_tz":-120,"elapsed":3243,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}},"outputId":"45625225-8bab-4b9f-cae7-448ca5518529"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'arabert'...\n","remote: Enumerating objects: 569, done.\u001b[K\n","remote: Counting objects: 100% (34/34), done.\u001b[K\n","remote: Compressing objects: 100% (11/11), done.\u001b[K\n","remote: Total 569 (delta 26), reused 24 (delta 23), pack-reused 535\u001b[K\n","Receiving objects: 100% (569/569), 9.12 MiB | 5.47 MiB/s, done.\n","Resolving deltas: 100% (327/327), done.\n"]}]},{"cell_type":"code","source":["%pip install pyarabic"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04V186hR-jIj","executionInfo":{"status":"ok","timestamp":1657471255383,"user_tz":-120,"elapsed":4608,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}},"outputId":"6de8258b-97e2-4331-dc9c-f075f40fc0b9"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyarabic\n","  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n","\u001b[K     |████████████████████████████████| 126 kB 31.0 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from pyarabic) (1.15.0)\n","Installing collected packages: pyarabic\n","Successfully installed pyarabic-0.6.15\n"]}]},{"cell_type":"code","source":["from arabert.preprocess import ArabertPreprocessor\n","model_name=\"bert-large-arabertv2\"\n","arabert_prep = ArabertPreprocessor(model_name=model_name)\n","\n","text = \"ولن نبالغ إذا قلنا إن هاتف أو كمبيوتر المكتب في زمننا هذا ضروري\"\n","arabert_prep.preprocess(text)"],"metadata":{"id":"T2ALkZbd5LFk","executionInfo":{"status":"error","timestamp":1657397706169,"user_tz":-120,"elapsed":32,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}},"outputId":"5feb3a56-289d-4dfb-8256-7840451d1664","colab":{"base_uri":"https://localhost:8080/","height":377}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:root:farasapy is not installed, you want be able to process text for AraBERTv1 and v2. Install it using: pip install farasapy\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-608aa97a636e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ولن نبالغ إذا قلنا إن هاتف أو كمبيوتر المكتب في زمننا هذا ضروري\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0marabert_prep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/arabert/preprocess.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSECOND_GEN_MODELS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_v3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/arabert/preprocess.py\u001b[0m in \u001b[0;36m_preprocess_v2\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfarasa_segmenter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_farasa_segment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'ArabertPreprocessor' object has no attribute 'farasa_segmenter'"]}]},{"cell_type":"code","source":["general_filter = lambda x: re.sub(r'([@A-Za-z0-9ـــــــــــــ]+)|[^\\w\\s]|#|http\\S+|[\\s]{2,}', '', x)\n","_filter = lambda x: re.sub(r'_', ' ', x)\n","rem_repeated_letters = lambda x: x[0:2] + ''.join([x[i] for i in range(2, len(x)) if x[i]!=x[i-1] or x[i]!=x[i-2]])\n","heart_emotion_translate = lambda x: re.sub(r'[♥|❤️|❤|♡]+', 'قلب', x)\n","happy_emotion_translate = lambda x: re.sub(r'(\\^[._]?\\^)+', 'سعيد', x)\n","sad_emotion_translate = lambda x: re.sub(r'(-[._]?-)+', 'حزين', x)\n","\n","def pre_process(data_frame):\n","  data_frame_x = data_frame.copy()\n","  data_frame_x[\"araBERT_prep\"] = data_frame_x.tweet.apply(lambda x: x[2:-2])\n","  data_frame_x[\"araBERT_prep\"] = data_frame_x[\"araBERT_prep\"].apply(heart_emotion_translate)\n","  data_frame_x[\"araBERT_prep\"] = data_frame_x[\"araBERT_prep\"].apply(happy_emotion_translate)\n","  data_frame_x[\"araBERT_prep\"] = data_frame_x[\"araBERT_prep\"].apply(sad_emotion_translate)\n","  data_frame_x[\"araBERT_prep\"] = data_frame_x[\"araBERT_prep\"].apply(general_filter)\n","  data_frame_x[\"araBERT_prep\"] = data_frame_x[\"araBERT_prep\"].apply(_filter)\n","  data_frame_x[\"araBERT_prep\"] = data_frame_x[\"araBERT_prep\"].apply(rem_repeated_letters) \n","  data_frame_x[\"araBERT_prep\"] = data_frame_x[\"araBERT_prep\"].apply(lambda x: arabert_prep.preprocess(x))\n","  data_frame_x[\"araBERT_prep\"] = data_frame_x[\"araBERT_prep\"].apply(lambda x: tokenizer(x).tokens())\n","  return data_frame_x\n"],"metadata":{"id":"DQTQil3w-52f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pre_process(data_frame)\n"],"metadata":{"id":"fCCzdghOCe3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"O2wZYI6QCi7f","executionInfo":{"status":"ok","timestamp":1657321833566,"user_tz":-120,"elapsed":8,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}},"outputId":"3c36988a-61bf-4380-9200-37c44b53a026"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                  tweet class  \\\n","0     ' #علمتني_الحياه أن الذين يعيشون على الأرض ليس...   pos   \n","1     ' #ميري_كرسمس كل سنة وانتم طيبين http://t.co/n...   pos   \n","2                             ' و انتهى مشوار الخواجة '   neg   \n","3                      ' مش عارف ابتدى مذاكره منين :/ '   neg   \n","4     ' @mskhafagi  إختصروا الطريق بدلا من إختيار ال...   neg   \n","...                                                 ...   ...   \n","2054  ' @wasfa_N الجمال مبيحتاح اي مكياج لناعم وله خ...   neu   \n","2055  ' @TheMurexDor نتمني وجود الفنانة رنا سماحة اف...   neu   \n","2056  ' ولد الهدى فالكائنات ضياء .. وفم الزمان تبسم ...   pos   \n","2057  ' @mohamed71944156 @samarroshdy1 انت متناقض جد...   neg   \n","2058  ' منطقة السيدة زينب ليلة المولد @ مسجد السيدة ...   neu   \n","\n","                                           araBERT_prep  \n","0     [[CLS], علمتني, ال+, حياه, أن, الذين, يعيش, +و...  \n","1     [[CLS], ميري, كرس, ##مس, كل, سن, +ة, و+, أنتم,...  \n","2        [[CLS], و, انتهى, مشوار, ال+, خواج, +ة, [SEP]]  \n","3     [[CLS], مش, عارف, ابتد, ##ى, مذاكر, +ه, من, +ي...  \n","4     [[CLS], إختصر, +وا, ال+, طريق, بدل, +ا, من, إخ...  \n","...                                                 ...  \n","2054  [[CLS], ال+, جمال, مبي, ##حت, ##اح, اي, مكياج,...  \n","2055  [[CLS], نتمني, وجود, ال+, فنان, +ة, رنا, سماح,...  \n","2056  [[CLS], ولد, ال+, هدى, ف+, ال+, كائن, +ات, ضيا...  \n","2057      [[CLS], أنت, متناقض, جد, +ا, يا, صلاح, [SEP]]  \n","2058  [[CLS], منطق, +ة, ال+, سيد, +ة, زينب, ليل, +ة,...  \n","\n","[2059 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-56dbbf94-c35f-4442-aacc-df6ae3e60b1f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet</th>\n","      <th>class</th>\n","      <th>araBERT_prep</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>' #علمتني_الحياه أن الذين يعيشون على الأرض ليس...</td>\n","      <td>pos</td>\n","      <td>[[CLS], علمتني, ال+, حياه, أن, الذين, يعيش, +و...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>' #ميري_كرسمس كل سنة وانتم طيبين http://t.co/n...</td>\n","      <td>pos</td>\n","      <td>[[CLS], ميري, كرس, ##مس, كل, سن, +ة, و+, أنتم,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>' و انتهى مشوار الخواجة '</td>\n","      <td>neg</td>\n","      <td>[[CLS], و, انتهى, مشوار, ال+, خواج, +ة, [SEP]]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>' مش عارف ابتدى مذاكره منين :/ '</td>\n","      <td>neg</td>\n","      <td>[[CLS], مش, عارف, ابتد, ##ى, مذاكر, +ه, من, +ي...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>' @mskhafagi  إختصروا الطريق بدلا من إختيار ال...</td>\n","      <td>neg</td>\n","      <td>[[CLS], إختصر, +وا, ال+, طريق, بدل, +ا, من, إخ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2054</th>\n","      <td>' @wasfa_N الجمال مبيحتاح اي مكياج لناعم وله خ...</td>\n","      <td>neu</td>\n","      <td>[[CLS], ال+, جمال, مبي, ##حت, ##اح, اي, مكياج,...</td>\n","    </tr>\n","    <tr>\n","      <th>2055</th>\n","      <td>' @TheMurexDor نتمني وجود الفنانة رنا سماحة اف...</td>\n","      <td>neu</td>\n","      <td>[[CLS], نتمني, وجود, ال+, فنان, +ة, رنا, سماح,...</td>\n","    </tr>\n","    <tr>\n","      <th>2056</th>\n","      <td>' ولد الهدى فالكائنات ضياء .. وفم الزمان تبسم ...</td>\n","      <td>pos</td>\n","      <td>[[CLS], ولد, ال+, هدى, ف+, ال+, كائن, +ات, ضيا...</td>\n","    </tr>\n","    <tr>\n","      <th>2057</th>\n","      <td>' @mohamed71944156 @samarroshdy1 انت متناقض جد...</td>\n","      <td>neg</td>\n","      <td>[[CLS], أنت, متناقض, جد, +ا, يا, صلاح, [SEP]]</td>\n","    </tr>\n","    <tr>\n","      <th>2058</th>\n","      <td>' منطقة السيدة زينب ليلة المولد @ مسجد السيدة ...</td>\n","      <td>neu</td>\n","      <td>[[CLS], منطق, +ة, ال+, سيد, +ة, زينب, ليل, +ة,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2059 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56dbbf94-c35f-4442-aacc-df6ae3e60b1f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-56dbbf94-c35f-4442-aacc-df6ae3e60b1f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-56dbbf94-c35f-4442-aacc-df6ae3e60b1f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["from sklearn import preprocessing\n","\n","araBERT_prep = df[\"araBERT_prep\"]\n","labels = df[\"class\"]\n","\n","# Apply label encoding over the labels\n","le = preprocessing.LabelEncoder()\n","Encodedlabels =le.fit_transform(labels)"],"metadata":{"id":"ilItbiZ61hGT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Encodedlabels\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jFwzO7StaXT0","outputId":"cd3eaaa3-10f7-48da-f954-eb796cccfc0d","executionInfo":{"status":"ok","timestamp":1657321844139,"user_tz":-120,"elapsed":263,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 2, 0, ..., 2, 0, 1])"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","source":["# Padding and attention mask"],"metadata":{"id":"GO6bLSh2I5GL"}},{"cell_type":"code","source":["from keras.preprocessing.sequence import pad_sequences\n","\n","# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n","# In the original paper, the authors used a length of 512.\n","MAX_LEN = 128\n","# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in araBERT_prep]\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask)"],"metadata":{"id":"onXUUeTK1Q4i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(input_ids[0])\n","print(attention_masks[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l-tjO1PLbPQh","outputId":"01d13c7f-466f-47a8-a46e-fa317dbe79e3","executionInfo":{"status":"ok","timestamp":1657321853564,"user_tz":-120,"elapsed":233,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[   33 31274    20 38652   302   801  2682    26   306    20  1155   682\n","    24 13030    12   406  3027   416  7891    24    29   416 13705    24\n","    13   363  4680   302 19779   306   736   290  1435   826  1707    16\n","   315    25   289    25    34     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0]\n","[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"]}]},{"cell_type":"markdown","source":["# To tensors"],"metadata":{"id":"Q72BkkMNI3aW"}},{"cell_type":"code","source":["import tensorflow as tf\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","\n","# Use train_test_split to split our data into train and validation sets for training\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, Encodedlabels, \n","                                                            random_state=2018, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                             random_state=2018, test_size=0.1)\n","# Convert all of our data into torch tensors, the required datatype for our model\n","\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)\n","# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n","batch_size = 16\n","\n","# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n","# with an iterator the entire dataset does not need to be loaded into memory\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_dataloader = DataLoader(validation_data, batch_size=batch_size)"],"metadata":{"id":"xxIrSSWV2DDP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for batch in train_dataloader:\n","  print(batch)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IXT5AmKLbyZR","outputId":"d9556d41-585c-4cef-c6ea-4abaa5ea9ba7","executionInfo":{"status":"ok","timestamp":1657321896407,"user_tz":-120,"elapsed":2,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[tensor([[   33, 52611, 11685,  ...,     0,     0,     0],\n","        [   33,   947, 27342,  ...,     0,     0,     0],\n","        [   33,  8773,    12,  ...,     0,     0,     0],\n","        ...,\n","        [   33, 13117,    20,  ...,     0,     0,     0],\n","        [   33,  1303, 23937,  ...,     0,     0,     0],\n","        [   33, 20779, 41406,  ...,     0,     0,     0]]), tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n","        [1., 1., 1.,  ..., 0., 0., 0.],\n","        [1., 1., 1.,  ..., 0., 0., 0.],\n","        ...,\n","        [1., 1., 1.,  ..., 0., 0., 0.],\n","        [1., 1., 1.,  ..., 0., 0., 0.],\n","        [1., 1., 1.,  ..., 0., 0., 0.]]), tensor([0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 1])]\n"]}]},{"cell_type":"markdown","source":["# Set optimizer parameters"],"metadata":{"id":"Ao7E_9NgI0yg"}},{"cell_type":"code","source":["import torch.optim as optim\n","\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'gamma', 'beta']\n","optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay_rate': 0.01},\n","                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay_rate': 0.0}]\n","# This variable contains all of the hyperparemeter information our training loop needs\n","#optimizer = BertAdam(optimizer_grouped_parameters,lr=2e-5,warmup=.1)\n","optimizer = optim.AdamW(optimizer_grouped_parameters,lr=2e-5)"],"metadata":{"id":"CBOHOHQn3CJj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loss = model(train_data[:1][0], attention_mask=train_data[:1][1], labels=train_data[:1][2])\n","# loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","# loss"],"metadata":{"id":"NmyUEzpf7e-C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"Kg_r0FF_IzfA"}},{"cell_type":"code","source":["from tqdm import tqdm, trange\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","t = []\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Store our loss and accuracy for plotting\n","train_loss_set = []\n","\n","# Number of training epochs \n","epochs = 5\n","\n","# Transfer the model to GPU\n","model.to(device)\n","\n","# trange is a tqdm wrapper around the normal python range\n","for _ in trange(epochs, desc=\"Epoch\"):\n","  \n","  \n","  # Training\n","  \n","  # Set our model to training mode (as opposed to evaluation mode)\n","  model.train()\n","  \n","  # Tracking variables\n","  tr_loss = 0\n","  nb_tr_examples, nb_tr_steps = 0, 0\n","  \n","  # Train the data for one epoch\n","  for step, batch in enumerate(train_dataloader):\n","    # Add batch to GPU\n","    b_input_ids, b_input_mask, b_labels = batch\n","\n","    # Clear out the gradients (by default they accumulate)\n","    optimizer.zero_grad()\n","\n","    # Forward pass\n","    loss = model(b_input_ids.to(device), token_type_ids=None, attention_mask=b_input_mask.to(device), labels=b_labels.to(device))[\"loss\"]\n","    train_loss_set.append(loss.item())\n","\n","    # Backward pass\n","    loss.backward()\n","    \n","    # Update parameters and take a step using the computed gradient\n","    optimizer.step()\n","    \n","    \n","    # Update tracking variables\n","    tr_loss += loss.item()\n","    nb_tr_examples += b_input_ids.size(0)\n","    nb_tr_steps += 1\n","\n","  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","    \n","  # Validation\n","\n","  # Put model in evaluation mode to evaluate loss on the validation set\n","  model.eval()\n","\n","  # Tracking variables \n","  eval_loss, eval_accuracy = 0, 0\n","  nb_eval_steps, nb_eval_examples = 0, 0\n","\n","  # Evaluate data for one epoch\n","  for batch in validation_dataloader:\n","    # Add batch to GPU\n","    # batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","    with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      logits = model(b_input_ids.to(device), token_type_ids=None, attention_mask=b_input_mask.to(device))\n","    \n","    # Move logits and labels to CPU\n","    logits = logits[\"logits\"].detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    \n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","\n","  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","  "],"metadata":{"id":"vDUxsUyL3Lmy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8cb48f53-5d73-4b60-c8a2-9ef729990df1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\rEpoch:   0%|          | 0/10 [00:00<?, ?it/s]"]}]},{"cell_type":"markdown","source":["# Apply predicition over the submission dataset"],"metadata":{"id":"ACCjdMIxPdoc"}},{"cell_type":"code","source":["\n","#============= Read CSV and apply data preperation =============#\n","df_submit = pd.read_csv(\"/content/drive/MyDrive/DEBI/uottawa/Ds applications/kaggle competition/test.csv\")\n","\n","# clean-up: remove #tags, http links and special symbols\n","df_submit.tweet = df_submit.tweet.apply(lambda x: x[2:-2])\n","df_submit.tweet = df_submit.tweet.apply(lambda x: re.sub(r'http\\S+', '', x))\n","df_submit.tweet = df_submit.tweet.apply(lambda x: re.sub(r'[@|#]\\S*', '', x))\n","df_submit.tweet = df_submit.tweet.apply(lambda x: re.sub(r'\"+', '', x))\n","\n","# Remove arabic signs\n","df_submit.tweet = df_submit.tweet.apply(lambda x: re.sub(r'([@A-Za-z0-9_ـــــــــــــ]+)|[^\\w\\s]|#|http\\S+', '', x))\n","\n","# Remove repeated letters like \"الللللللللللللللله\" to \"الله\"\n","df_submit.tweet = df_submit.tweet.apply(lambda x: x[0:2] + ''.join([x[i] for i in range(2, len(x)) if x[i]!=x[i-1] or x[i]!=x[i-2]]))\n","\n","# Tokenize the sentences using bert tokenizer\n","df_submit[\"araBERT_prep\"] = df_submit.tweet.apply(lambda x: tokenizer(x).tokens())"],"metadata":{"id":"DHlHPF8wPhx8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_submit = pd.read_csv(\"/content/drive/MyDrive/DEBI/uottawa/Ds applications/kaggle competition/test.csv\")\n","\n","df_submit.tweet = df_submit.tweet.apply(lambda x: x[2:-2]).apply(heart_emotion_translate).apply(happy_emotion_translate).apply(sad_emotion_translate).apply(general_filter).apply(rem_repeated_letters).apply(lem_stopwords_tokenize)\n","\n","df_submit[\"araBERT_prep\"] = df_submit.tweet.apply(lambda x:tokenizer(x).tokens())# list(filter(lambda x: \"#\" not in x, tokenizer(x).tokens())))\n"],"metadata":{"id":"r3oZNpodblu7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df_submit[\"araBERT_prep\"] = pre_process(df_submit)\n","df_submit.head()\n","araBERT_prep_submit = df_submit[\"araBERT_prep\"]"],"metadata":{"id":"YiCekroGRFKG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n","# In the original paper, the authors used a length of 512.\n","MAX_LEN = 128\n","# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n","input_ids_submit = [tokenizer.convert_tokens_to_ids(x) for x in araBERT_prep_submit]\n","# Pad our input tokens\n","input_ids_submit = pad_sequences(input_ids_submit, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks_submit = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids_submit:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks_submit.append(seq_mask)"],"metadata":{"id":"KJ3vtlYkRTGb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert all of our data into torch tensors, the required datatype for our model\n","inputs_submit = torch.tensor(input_ids_submit)\n","masks_submit = torch.tensor(attention_masks_submit)\n","\n","# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n","# with an iterator the entire dataset does not need to be loaded into memory\n","batch_size = 16\n","submit_data = TensorDataset(inputs_submit, masks_submit)\n","\n","# do not use shuffle, we need the preds to be in same order\n","submit_dataloader = DataLoader(submit_data, batch_size=batch_size)#, shuffle=True)"],"metadata":{"id":"58982HhxRLUD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# input, masks = next(iter(submit_dataloader))\n","# out = model(input, attention_mask=masks)[\"logits\"]\n","\n","# pred_flat = np.argmax(out.detach().numpy(), axis=1).flatten()\n","# le.inverse_transform(pred_flat)"],"metadata":{"id":"0NfUlXyySAh8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Put the model in an evaluation state\n","model.eval()\n","\n","# Transfer model to GPU\n","model.to(device)\n","\n","outputs = []\n","for input, masks in submit_dataloader:\n","  torch.cuda.empty_cache() # empty the gpu memory\n","\n","  # Transfer the batch to gpu\n","  input = input.to(device)\n","  masks = masks.to(device)\n","\n","  # Run inference on the batch\n","  output = model(input, attention_mask=masks)[\"logits\"]\n","\n","  # Transfer the output to CPU again and convert to numpy\n","  output = output.cpu().detach().numpy()\n","\n","  # Store the output in a list\n","  outputs.append(output)\n","\n","# Concatenate all the lists within the list into one list\n","outputs = [x for y in outputs for x in y]\n","\n","# Inverse transform the label encoding\n","pred_flat = np.argmax(outputs, axis=1).flatten()\n","output_labels = le.inverse_transform(pred_flat)"],"metadata":{"id":"aZm0cffsRrY9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission = pd.DataFrame({\"Id\":np.arange(1, len(output_labels)+1), \"class\":output_labels})\n","# save (submission)\n","submission.to_csv(\"/content/drive/MyDrive/DEBI/uottawa/Ds applications/kaggle competition/submission3.csv\", index=False)"],"metadata":{"id":"5oOuGfmaSza0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = ' \"\"\"@HHesho لأ بن الصرمة دا لسه فى الفيلدز ، Euler in ODE :D بس non linear وحاجه بلاعات خالص\"\"\"\" '\n","arabert_prep.preprocess(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"25gP0f9TAyRe","executionInfo":{"status":"ok","timestamp":1657320812209,"user_tz":-120,"elapsed":277,"user":{"displayName":"Khaled El-Saka","userId":"03114099625279850307"}},"outputId":"6c3defb4-4819-4b98-a476-6b2abb44912a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[بريد] لأ بن ال+ صرم +ة دا لس +ه في ال+ فيلدز ، Euler in ODE : D بس non linear و+ حاج +ه ب+ لاع +ات خالص \" \"'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":[""],"metadata":{"id":"J1JoqpwpBKr2"},"execution_count":null,"outputs":[]}]}