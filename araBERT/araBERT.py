# -*- coding: utf-8 -*-
"""araBERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I_8J7mxEScw1TPrkO3YGOAqD3cpCmQ9m
"""

# Commented out IPython magic to ensure Python compatibility.

# %pip install transformers

# Commented out IPython magic to ensure Python compatibility.
# %store transformers

from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification

#============= Initialize Arabic Bert =============#
# tokenizer = AutoTokenizer.from_pretrained("aubmindlab/bert-large-arabertv2")
#model = AutoModelForSequenceClassification.from_pretrained("aubmindlab/bert-large-arabertv2", num_labels=3)

# from google.colab import drive
# drive.mount('/content/drive')

directory = "D:/DEBI/Uottawa/Data Science Applications/kaggle/"
# tokenizer.save_pretrained(directory+"araBERT/araBERT_tokenizer")
# model.save_pretrained(directory+"araBERT/araBERT_Model")

"""# Load data"""

import numpy as np
import pandas as pd
import re

from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
directory = "D:/DEBI/Uottawa/Data Science Applications/kaggle/"
tokenizer = AutoTokenizer.from_pretrained(directory+"araBERT/araBERT_tokenizer")
model = AutoModelForSequenceClassification.from_pretrained(directory+"araBERT/araBERT_Model")

data_frame = pd.read_csv(directory+"araBERT/balanced_data.csv")

print(data_frame["class"].value_counts())


# Commented out IPython magic to ensure Python compatibility.
# %pip install farasapy


# Commented out IPython magic to ensure Python compatibility.
# %pip install pyarabic

from preprocess import ArabertPreprocessor
# from farasa.segmenter import FarasaSegmenter 
# segmenter = FarasaSegmenter(interactive=True)

model_name="bert-large-arabertv2"
arabert_prep = ArabertPreprocessor(model_name=model_name)
text = "ولن_نبالغ إذا قلنا إن هاتف أو كمبيوتر المكتب في زمننا هذا ضروري''"""

print(arabert_prep.preprocess(text.strip("\'")))
# segmenter.segment(text)






general_filter = lambda x: re.sub(r'([@A-Za-z0-9ـــــــــــــ]+)|[^\w\s]|#|http\S+|[\s]{2,}', '', x)
_filter = lambda x: re.sub(r'_', ' ', x)
rem_repeated_letters = lambda x: x[0:2] + ''.join([x[i] for i in range(2, len(x)) if x[i]!=x[i-1] or x[i]!=x[i-2]])
heart_emotion_translate = lambda x: re.sub(r'[♥|❤️|❤|♡]+', 'قلب', x)
happy_emotion_translate = lambda x: re.sub(r'(\^[._]?\^)+', 'سعيد', x)
sad_emotion_translate = lambda x: re.sub(r'(-[._]?-)+', 'حزين', x)

def pre_process(data_frame):
  data_frame_x = data_frame.copy()
  preProcessed = []   
  for x in data_frame_x.tweet:
      x = x.strip("\'")
      x = heart_emotion_translate(x)
      x = happy_emotion_translate(x)
      x = sad_emotion_translate(x)
      x = general_filter(x)
      x = _filter(x)
      x = rem_repeated_letters(x)
      x = arabert_prep.preprocess(x)
      x = tokenizer(x).tokens()
      preProcessed.append(x)
  data_frame_x["araBERT"] = preProcessed
  '''     
  # data_frame_x["araBERT_prep"] = data_frame_x.tweet.apply(lambda x: x[2:-2])
  data_frame_x["araBERT_prep"] = data_frame_x.tweet.apply(heart_emotion_translate)
  data_frame_x["araBERT_prep"] = data_frame_x["araBERT_prep"].apply(happy_emotion_translate)
  data_frame_x["araBERT_prep"] = data_frame_x["araBERT_prep"].apply(sad_emotion_translate)
  data_frame_x["araBERT_prep"] = data_frame_x["araBERT_prep"].apply(general_filter)
  data_frame_x["araBERT_prep"] = data_frame_x["araBERT_prep"].apply(_filter)
  data_frame_x["araBERT_prep"] = data_frame_x["araBERT_prep"].apply(rem_repeated_letters) 
  data_frame_x["araBERT_prep"] = data_frame_x["araBERT_prep"].apply(lambda x: arabert_prep.preprocess(x))
  data_frame_x["araBERT_prep"] = data_frame_x["araBERT_prep"].apply(lambda x: tokenizer(x).tokens())
  '''
  return data_frame_x

df = pre_process(data_frame)

df

from sklearn import preprocessing

araBERT_prep = df["araBERT"]
labels = df["class"]

# Apply label encoding over the labels
le = preprocessing.LabelEncoder()
Encodedlabels =le.fit_transform(labels)

Encodedlabels

"""# Padding and attention mask"""

from keras.preprocessing.sequence import pad_sequences

# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. 
# In the original paper, the authors used a length of 512.
MAX_LEN = 64
# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary
input_ids = [tokenizer.convert_tokens_to_ids(x) for x in araBERT_prep]
# Pad our input tokens
input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")
# Create attention masks
attention_masks = []

# Create a mask of 1s for each token followed by 0s for padding
for seq in input_ids:
  seq_mask = [float(i>0) for i in seq]
  attention_masks.append(seq_mask)

print(input_ids[0])
print(attention_masks[0])

"""# To tensors"""

import torch
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
import numpy as np 
# Use train_test_split to split our data into train and validation sets for training
percentage = 0.15
validationLen = round(percentage *len(input_ids))

train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, Encodedlabels,
                                        random_state=2018, test_size=0.3 ,stratify=Encodedlabels)
#input_ids[:-validationLen],input_ids[-validationLen:],Encodedlabels[:-validationLen],Encodedlabels[-validationLen:]

train_masks, validation_masks, _,_ = train_test_split(attention_masks, input_ids,
                                        random_state=2018, test_size=0.3,stratify=Encodedlabels)
#attention_masks[:-validationLen],attention_masks[-validationLen:]
#
# Convert all of our data into torch tensors, the required datatype for our model
unique, counts = np.unique(train_masks, return_counts=True)
train_inputs = torch.tensor(train_inputs,dtype=torch.long)

validation_inputs = torch.tensor(validation_inputs,dtype=torch.long)
train_labels = torch.tensor(train_labels,dtype=torch.long)
validation_labels = torch.tensor(validation_labels,dtype=torch.long)
train_masks = torch.tensor(train_masks,dtype=torch.long)
validation_masks = torch.tensor(validation_masks,dtype=torch.long)
# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32
batch_size = 4

# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, 
# with an iterator the entire dataset does not need to be loaded into memory

train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)

validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
validation_dataloader = DataLoader(validation_data, batch_size=batch_size)

for batch in train_dataloader:
  print(batch)
  break

"""# Set optimizer parameters"""

import torch.optim as optim

param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'gamma', 'beta']
optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay_rate': 0.01},
                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay_rate': 0.0}]
# This variable contains all of the hyperparemeter information our training loop needs
#optimizer = BertAdam(optimizer_grouped_parameters,lr=2e-5,warmup=.1)
optimizer = optim.AdamW(optimizer_grouped_parameters,lr=2e-5)

# loss = model(train_data[:1][0], attention_mask=train_data[:1][1], labels=train_data[:1][2])
# loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
# loss

"""# Training"""

from tqdm import tqdm, trange
# Function to calculate the accuracy of our predictions vs labels
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)
t = []
#device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device = torch.device("cpu")
# Store our loss and accuracy for plotting
train_loss_set = []

# Number of training epochs 
epochs = 5

# Transfer the model to GPU
model.to(device)

# trange is a tqdm wrapper around the normal python range
for _ in trange(epochs, desc="Epoch"):
  
  
  # Training
  
  # Set our model to training mode (as opposed to evaluation mode)
  model.train()
  
  # Tracking variables
  tr_loss = 0
  nb_tr_examples, nb_tr_steps = 0, 0
  
  # Train the data for one epoch
  for step, batch in enumerate(train_dataloader):
    # Add batch to GPU
    b_input_ids, b_input_mask, b_labels = batch

    # Clear out the gradients (by default they accumulate)
    optimizer.zero_grad()

    # Forward pass 
    loss = model(b_input_ids.to(device), token_type_ids=None, attention_mask=b_input_mask.to(device), labels=b_labels.to(device))["loss"]
    train_loss_set.append(loss.item())

    # Backward pass
    loss.backward()
    
    # Update parameters and take a step using the computed gradient
    optimizer.step()
    
    
    # Update tracking variables
    tr_loss += loss.item()
    nb_tr_examples += b_input_ids.size(0)
    nb_tr_steps += 1

  print("Train loss: {}".format(tr_loss/nb_tr_steps))
    
  # Validation

  # Put model in evaluation mode to evaluate loss on the validation set
  model.eval()

  # Tracking variables 
  eval_loss, eval_accuracy = 0, 0
  nb_eval_steps, nb_eval_examples = 0, 0

  # Evaluate data for one epoch
  for batch in validation_dataloader:
    # Add batch to GPU
    # batch = tuple(t.to(device) for t in batch)
    # Unpack the inputs from our dataloader
    b_input_ids, b_input_mask, b_labels = batch
    # Telling the model not to compute or store gradients, saving memory and speeding up validation
    with torch.no_grad():
      # Forward pass, calculate logit predictions
      logits = model(b_input_ids.to(device), token_type_ids=None, attention_mask=b_input_mask.to(device))
    
    # Move logits and labels to CPU
    logits = logits["logits"].detach().cpu().numpy()
    label_ids = b_labels.to('cpu').numpy()

    tmp_eval_accuracy = flat_accuracy(logits, label_ids)
    
    eval_accuracy += tmp_eval_accuracy
    nb_eval_steps += 1

  print("Validation Accuracy: {}".format(eval_accuracy/nb_eval_steps))

"""# Apply predicition over the submission dataset"""

#============= Read CSV and apply data preperation =============#
df_submit = pd.read_csv("/content/drive/MyDrive/DEBI/uottawa/Ds applications/kaggle competition/test.csv")

# clean-up: remove #tags, http links and special symbols
df_submit.tweet = df_submit.tweet.apply(lambda x: x[2:-2])
df_submit.tweet = df_submit.tweet.apply(lambda x: re.sub(r'http\S+', '', x))
df_submit.tweet = df_submit.tweet.apply(lambda x: re.sub(r'[@|#]\S*', '', x))
df_submit.tweet = df_submit.tweet.apply(lambda x: re.sub(r'"+', '', x))

# Remove arabic signs
df_submit.tweet = df_submit.tweet.apply(lambda x: re.sub(r'([@A-Za-z0-9_ـــــــــــــ]+)|[^\w\s]|#|http\S+', '', x))

# Remove repeated letters like "الللللللللللللللله" to "الله"
df_submit.tweet = df_submit.tweet.apply(lambda x: x[0:2] + ''.join([x[i] for i in range(2, len(x)) if x[i]!=x[i-1] or x[i]!=x[i-2]]))

# Tokenize the sentences using bert tokenizer
df_submit["araBERT_prep"] = df_submit.tweet.apply(lambda x: tokenizer(x).tokens())

df_submit = pd.read_csv("/content/drive/MyDrive/DEBI/uottawa/Ds applications/kaggle competition/test.csv")

#df_submit.tweet = df_submit.tweet.apply(lambda x: x[2:-2]).apply(heart_emotion_translate).apply(happy_emotion_translate).apply(sad_emotion_translate).apply(general_filter).apply(rem_repeated_letters).apply(lem_stopwords_tokenize)

df_submit["araBERT_prep"] = df_submit.tweet.apply(lambda x:tokenizer(x).tokens())# list(filter(lambda x: "#" not in x, tokenizer(x).tokens())))

# df_submit["araBERT_prep"] = pre_process(df_submit)
df_submit.head()
araBERT_prep_submit = df_submit["araBERT_prep"]

# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. 
# In the original paper, the authors used a length of 512.
MAX_LEN = 128
# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary
input_ids_submit = [tokenizer.convert_tokens_to_ids(x) for x in araBERT_prep_submit]
# Pad our input tokens
input_ids_submit = pad_sequences(input_ids_submit, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")
# Create attention masks
attention_masks_submit = []

# Create a mask of 1s for each token followed by 0s for padding
for seq in input_ids_submit:
  seq_mask = [float(i>0) for i in seq]
  attention_masks_submit.append(seq_mask)

# Convert all of our data into torch tensors, the required datatype for our model
inputs_submit = torch.tensor(input_ids_submit)
masks_submit = torch.tensor(attention_masks_submit)

# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, 
# with an iterator the entire dataset does not need to be loaded into memory
batch_size = 16
submit_data = TensorDataset(inputs_submit, masks_submit)

# do not use shuffle, we need the preds to be in same order
submit_dataloader = DataLoader(submit_data, batch_size=batch_size)#, shuffle=True)

# input, masks = next(iter(submit_dataloader))
# out = model(input, attention_mask=masks)["logits"]

# pred_flat = np.argmax(out.detach().numpy(), axis=1).flatten()
# le.inverse_transform(pred_flat)

# Put the model in an evaluation state
model.eval()

# Transfer model to GPU
model.to(device)

outputs = []
for input, masks in submit_dataloader:
  torch.cuda.empty_cache() # empty the gpu memory

  # Transfer the batch to gpu
  input = input.to(device)
  masks = masks.to(device)

  # Run inference on the batch
  output = model(input, attention_mask=masks)["logits"]

  # Transfer the output to CPU again and convert to numpy
  output = output.cpu().detach().numpy()

  # Store the output in a list
  outputs.append(output)

# Concatenate all the lists within the list into one list
outputs = [x for y in outputs for x in y]

# Inverse transform the label encoding
pred_flat = np.argmax(outputs, axis=1).flatten()
output_labels = le.inverse_transform(pred_flat)

submission = pd.DataFrame({"Id":np.arange(1, len(output_labels)+1), "class":output_labels})
# save (submission)
submission.to_csv("/content/drive/MyDrive/DEBI/uottawa/Ds applications/kaggle competition/submission3.csv", index=False)

text = ' """@HHesho لأ بن الصرمة دا لسه فى الفيلدز ، Euler in ODE :D بس non linear وحاجه بلاعات خالص"""" '
arabert_prep.preprocess(text)

